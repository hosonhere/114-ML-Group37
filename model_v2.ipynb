{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "current_dir = os.getcwd() # Get the current directory\n",
    "sys.path.append(current_dir) # Add the directory where the module is located\n",
    "\n",
    "# from pathlib import Path\n",
    "from yolov9_2.utils.general import non_max_suppression, scale_boxes\n",
    "# from utils.plots import Annotator, colors\n",
    "from yolov9_2.models.common import DetectMultiBackend\n",
    "# from utils.dataloaders import LoadImages\n",
    "from yolov9_2.utils.general import check_img_size\n",
    "from yolov9_2.utils.torch_utils import select_device\n",
    "from yolov9_2.utils.augmentations import (Albumentations, augment_hsv, classify_albumentations, classify_transforms, copy_paste,\n",
    "                                 letterbox, mixup, random_perspective)\n",
    "from yolov9_2.utils.plots import Annotator, colors\n",
    "from itertools import zip_longest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Data Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Helper function <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_index(char):\n",
    "    \"\"\"\n",
    "    Convert a letter or digit to its corresponding index.\n",
    "\n",
    "    Args:\n",
    "        char: str, the character to convert.\n",
    "\n",
    "    Returns:\n",
    "        int, the index of the character.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the character is unsupported.\n",
    "    \"\"\"\n",
    "    if char.isdigit():\n",
    "        return ord(char) - ord('0')  # Digits (0-9) -> 0-9\n",
    "    elif 'a' <= char <= 'z':\n",
    "        return ord(char) - ord('a') + 10  # Lowercase letters (a-z) -> 10-35\n",
    "    elif 'A' <= char <= 'Z':\n",
    "        return ord(char) - ord('A') + 36\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported character: {char}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Load datasets <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_dir, labels, limit, predict_split=1000, img_height=90, img_width=30, num_classes=62):\n",
    "    \"\"\"\n",
    "        Load images and labels from the specified directory.\n",
    "\n",
    "        Args:\n",
    "            image_dir: str, the directory containing the images.\n",
    "            labels: dict, a mapping from image filenames to labels.\n",
    "            limit: int, the maximum number of images to load.\n",
    "            predict_split: int, the number of images to reserve for prediction.\n",
    "            img_height: int, the height of the images.\n",
    "            img_width: int, the width of the images.\n",
    "            num_classes: int, the number of classes.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of NumPy arrays: (images, target_labels, predict_images, predict_labels)\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    target_labels = []\n",
    "    predict_images = [] \n",
    "    predict_labels = []  # Store labels for reserved images\n",
    "\n",
    "  \n",
    "    all_image_filenames = [f for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "  \n",
    "    for i, img_name in enumerate(all_image_filenames):\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "  \n",
    "    \n",
    "        label = labels[img_name]\n",
    "        img = load_img(img_path, color_mode=\"grayscale\", target_size=(img_height, img_width))\n",
    "        img_array = img_to_array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
    "            \n",
    "         \n",
    "        char_index = get_char_index(label)  # Convert char to index (0-61)\n",
    "        label_array = to_categorical(char_index, num_classes=num_classes)\n",
    "            \n",
    "           \n",
    "        if i < predict_split:\n",
    "                predict_images.append(img_array)\n",
    "                predict_labels.append(label_array)\n",
    "        else:\n",
    "                # Store in main arrays if within limit\n",
    "                if len(images) < limit:\n",
    "                    images.append(img_array)\n",
    "                    target_labels.append(label_array)\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    images = np.array(images)\n",
    "    target_labels = np.array(target_labels)\n",
    "    predict_images = np.array(predict_images)\n",
    "    predict_labels = np.array(predict_labels)\n",
    "   \n",
    "    return images, target_labels, predict_images, predict_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Loads Label.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels_from_csv(csv_path):\n",
    "    \"\"\"\n",
    "        Load image labels from a CSV file.\n",
    "\n",
    "        Args:\n",
    "            csv_path: str, the path to the CSV file.\n",
    "\n",
    "        Returns:\n",
    "            dict, a mapping from image filenames to labels.\n",
    "    \"\"\"\n",
    "    labels = {}\n",
    "    with open(csv_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            labels[row['image']] = row['label']\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Caller & Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = 90, 30  # Image dimensions (adjust for individual characters)\n",
    "num_classes = 62  # Number of classes (0-9, A-Z, a-z)\n",
    "\n",
    "training_data_path = \"dataset\\\\overlap\\\\train\\\\segment\"      # Directory containing images\n",
    "training_data_label_path = \"dataset\\\\overlap\\\\train\\\\labels\\\\segmented_labels.csv\"  # Path to the CSV file with labels\n",
    "validation_data_path = \"dataset\\\\overlap\\\\val\\\\segment\"  # Directory containing images\n",
    "validation_data_label_path = \"dataset\\\\overlap\\\\val\\\\labels\\\\segmented_labels.csv\"  # Path to the CSV file with labels\n",
    "\n",
    "# Load labels from CSV\n",
    "train_labels = load_labels_from_csv(training_data_label_path)\n",
    "\n",
    "# Load train images and labels\n",
    "X_train, y_train, _, _ = load_data(training_data_path, train_labels, limit=100000, predict_split=0, img_height=img_height, img_width=img_width, num_classes=num_classes)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(images, target_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load validation images and labels\n",
    "val_labels = load_labels_from_csv(validation_data_label_path)\n",
    "X_val, y_val, _, _ = load_data(validation_data_path, val_labels, limit=20000, predict_split=0, img_height=img_height, img_width=img_width, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.0) Test if your GPU is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "(100000, 90, 30, 1) (100000, 62)\n",
      "(20000, 90, 30, 1) (20000, 62)\n"
     ]
    }
   ],
   "source": [
    "### Testing if your GPU is available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "### print the shape of the training and validation data\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "# print(predict_images.shape, predict_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1563/1563 [==============================] - 28s 13ms/step - loss: 0.7168 - accuracy: 0.8249 - val_loss: 0.3637 - val_accuracy: 0.9216\n",
      "Epoch 2/20\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 0.3817 - accuracy: 0.9143 - val_loss: 0.3494 - val_accuracy: 0.9242\n",
      "Epoch 3/20\n",
      "1563/1563 [==============================] - 21s 13ms/step - loss: 0.3257 - accuracy: 0.9269 - val_loss: 0.3195 - val_accuracy: 0.9331\n",
      "Epoch 4/20\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.2864 - accuracy: 0.9341 - val_loss: 0.3226 - val_accuracy: 0.9363\n",
      "Epoch 5/20\n",
      "1563/1563 [==============================] - 22s 14ms/step - loss: 0.2553 - accuracy: 0.9402 - val_loss: 0.3254 - val_accuracy: 0.9330\n",
      "Epoch 6/20\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2269 - accuracy: 0.9446 - val_loss: 0.3388 - val_accuracy: 0.9365\n",
      "Epoch 7/20\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.2003 - accuracy: 0.9492 - val_loss: 0.3312 - val_accuracy: 0.9388\n",
      "Epoch 8/20\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1783 - accuracy: 0.9530 - val_loss: 0.3620 - val_accuracy: 0.9383\n",
      "Epoch 9/20\n",
      "1563/1563 [==============================] - 23s 15ms/step - loss: 0.1535 - accuracy: 0.9572 - val_loss: 0.3963 - val_accuracy: 0.9370\n",
      "Epoch 10/20\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1388 - accuracy: 0.9596 - val_loss: 0.4137 - val_accuracy: 0.9394\n",
      "Epoch 11/20\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 0.1253 - accuracy: 0.9620 - val_loss: 0.4212 - val_accuracy: 0.9383\n",
      "Epoch 12/20\n",
      "1563/1563 [==============================] - 24s 16ms/step - loss: 0.1102 - accuracy: 0.9660 - val_loss: 0.4551 - val_accuracy: 0.9374\n",
      "Epoch 13/20\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.1020 - accuracy: 0.9676 - val_loss: 0.4544 - val_accuracy: 0.9377\n",
      "Epoch 14/20\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 0.0924 - accuracy: 0.9701 - val_loss: 0.4619 - val_accuracy: 0.9392\n",
      "Epoch 15/20\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0856 - accuracy: 0.9719 - val_loss: 0.5055 - val_accuracy: 0.9377\n",
      "Epoch 16/20\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 0.0816 - accuracy: 0.9732 - val_loss: 0.5219 - val_accuracy: 0.9380\n",
      "Epoch 17/20\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.0743 - accuracy: 0.9756 - val_loss: 0.5406 - val_accuracy: 0.9362\n",
      "Epoch 18/20\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 0.0693 - accuracy: 0.9771 - val_loss: 0.5622 - val_accuracy: 0.9385\n",
      "Epoch 19/20\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 0.0686 - accuracy: 0.9768 - val_loss: 0.6062 - val_accuracy: 0.9384\n",
      "Epoch 20/20\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 0.0631 - accuracy: 0.9784 - val_loss: 0.5888 - val_accuracy: 0.9383\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.5888 - accuracy: 0.9383\n",
      "Validation accuracy: 0.94\n",
      "Validation loss: 0.59\n"
     ]
    }
   ],
   "source": [
    "input_shape = (90, 30, 1)  # Shape of the input image (for single character)\n",
    "input_img = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional layers #1\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "# Convolutional layers #2\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "# Convolutional layers #3\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "# Flatten\n",
    "x = Flatten()(x)\n",
    "# Dense and dropout layers\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# Single output layer for character classification\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_img, outputs=output)\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,            \n",
    "    batch_size=64,         \n",
    "    validation_data=(X_val, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f\"Validation accuracy: {accuracy:.2f}\")\n",
    "print(f\"Validation loss: {loss:.2f}\")\n",
    "\n",
    "model.save(\"captcha_model_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.1) Data Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prediction import init_process, process_single_image, sliding_predict_and_validate \n",
    "def preprocess_image(image, input_shape):\n",
    "    \"\"\"\n",
    "    Preprocess image for model prediction.\n",
    "    Args:\n",
    "        image: np.array, input image.\n",
    "        input_shape: tuple, (height, width) required by the model.\n",
    "\n",
    "    Returns:\n",
    "        np.array, preprocessed image.\n",
    "    \"\"\"\n",
    "    # Resize image to match model's expected sizing and scale pixel values\n",
    "    image = cv2.resize(image, (input_shape[1], input_shape[0]))\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "def repair_gray(frame, print_img=False):\n",
    "    # 将图像转换为灰度\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # gray = frame\n",
    "\n",
    "    # 进行二值化处理，将数字和背景区分开\n",
    "    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU) # this method can automatically find the best threshold value\n",
    "    # Other threshold methods\n",
    "    # _, binary = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    if print_img:\n",
    "        plt.imshow(binary, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('Binary Image')\n",
    "        plt.show()\n",
    "\n",
    "    # 使用形态学操作去除小噪声\n",
    "    # Erode\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    cleaned = cv2.erode(binary, kernel, iterations=1)\n",
    "    # Dilate\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    cleaned = cv2.dilate(cleaned, kernel, iterations=2)\n",
    "\n",
    "    # Remove lines\n",
    "    kernel = np.ones((2, 2), np.uint8)\n",
    "    cleaned = cv2.erode(cleaned, kernel, iterations=3)\n",
    "\n",
    "    # 反转图像，使数字为白色，背景为黑色\n",
    "    mask = cv2.bitwise_not(cleaned)\n",
    "    \n",
    "    # 检查生成的掩码\n",
    "    # cv2.imwrite(\"cleaned_mask.jpg\", mask)\n",
    "\n",
    "    # 使用掩码进行修复\n",
    "    # dst = cv2.inpaint(frame, mask, 18, cv2.INPAINT_TELEA)\n",
    "\n",
    "    # 保存修复后的图像\n",
    "    # cv2.imwrite(\"repaired_image.png\", dst)\n",
    "    dst = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "    return dst\n",
    "\n",
    "def to_black_and_white(image, threshold=200, print_img=False):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    _, binary_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "    binary_image = cv2.bitwise_not(binary_image)\n",
    "    if print_img:\n",
    "        plt.imshow(binary_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('Binary Image')\n",
    "        plt.show()\n",
    "\n",
    "    return binary_image\n",
    "\n",
    "def segment_using_column_scan(image, captcha_text, print_msg=False):\n",
    "    '''\n",
    "        Segment characters using column scanning method\n",
    "        \n",
    "        Args:\n",
    "            image: cv2 image (binary black and white)\n",
    "        \n",
    "        Returns:\n",
    "            character_images: list of segmented character images\n",
    "    '''\n",
    "    captcha_text_length = len(captcha_text)\n",
    "    if print_msg:\n",
    "        print(f'CAPTCHA text length: {captcha_text_length}')\n",
    "\n",
    "    # Convert image to binary\n",
    "    binary_image = image.copy()\n",
    "    # Reverse the color from background white to black, character black to white\n",
    "    # White: 255, Black: 0\n",
    "    binary_image = cv2.bitwise_not(binary_image)\n",
    "\n",
    "    # Get height and width of image\n",
    "    height, width = binary_image.shape\n",
    "    if print_msg:\n",
    "        print(f'height: {height}, width: {width}')\n",
    "    \n",
    "    # Initialize variables\n",
    "    char_start = None\n",
    "    character_images = []\n",
    "    \n",
    "    # 扫描每一列\n",
    "    for i, col in enumerate(range(width)):\n",
    "        column_sum = np.sum(binary_image[:, col])\n",
    "        # print(f'column {i+1} sum: {column_sum}')\n",
    "        # white: 0, black: 255\n",
    "        if column_sum > 255 * 1 and char_start is None:\n",
    "            char_start = col\n",
    "        # if column sum < 50, it may be end of character\n",
    "        elif column_sum < 255 * 1 and char_start is not None:\n",
    "\n",
    "            # That might be connected characters, so we need to use the threshold to split them\n",
    "            num_col = col - char_start\n",
    "            threshold = 50\n",
    "            average_col = num_col / threshold    \n",
    "            overlapping_col = 0\n",
    "\n",
    "            if average_col <= 1:\n",
    "                # Split character directly\n",
    "                char_image = binary_image[:, char_start:col]\n",
    "\n",
    "                # Calculate the width of the character\n",
    "                char_width = col - char_start\n",
    "                # If the width of the character is too small, it may be noise\n",
    "                if char_width > 10:\n",
    "                    character_images.append(cv2.bitwise_not(char_image))\n",
    "            else:\n",
    "                average_col = int(average_col)\n",
    "                # Split connected characters\n",
    "                for j in range(average_col):\n",
    "                    # Split character using threshold & add some overlapping\n",
    "                    if j == 0:\n",
    "                        char_image = binary_image[:, char_start + j * threshold : char_start + (j + 1) * threshold + overlapping_col]\n",
    "                    else:\n",
    "                        char_image = binary_image[:, char_start + j * threshold : char_start + (j + 1) * threshold + overlapping_col]\n",
    "                    character_images.append(cv2.bitwise_not(char_image))\n",
    "\n",
    "            char_start = None\n",
    "\n",
    "\n",
    "    # 检查最后一个字符是否未完成\n",
    "    if char_start is not None:\n",
    "        char_image = binary_image[:, char_start:width]\n",
    "        character_images.append(cv2.bitwise_not(char_image))\n",
    "\n",
    "    if len(character_images) != captcha_text_length:\n",
    "        if print_msg:\n",
    "            print(f'Warning: Number of segmented characters is not equal to CAPTCHA text length')\n",
    "\n",
    "        character_images = []\n",
    "    return character_images\n",
    "\n",
    "def preprocess_captcha_image(image_path, captcha_text='1234', print_img=False):\n",
    "    '''\n",
    "        Preprocess CAPTCHA image and segment characters\n",
    "\n",
    "        Args:\n",
    "            image_path: str, path to the image file\n",
    "        \n",
    "        Returns:\n",
    "            characters: list, list of segmented characters in image\n",
    "    '''\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    # Display the image\n",
    "    if print_img:\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title('Original CAPTCHA Image')\n",
    "        plt.show()\n",
    "\n",
    "    # Repair gray\n",
    "    preprocessed_image = repair_gray(image, print_img=print_img)\n",
    "    if print_img:\n",
    "        plt.imshow(preprocessed_image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title('After open CAPTCHA Image')\n",
    "        plt.show()\n",
    "\n",
    "    # Save the preprocessed image\n",
    "    # cv2.imwrite('preprocessed_image.png', preprocessed_image)\n",
    "\n",
    "    # Column scan method to segment characters\n",
    "    # Turn the image into binary format\n",
    "    preprocessed_image = cv2.cvtColor(preprocessed_image, cv2.COLOR_BGR2GRAY)\n",
    "    # characters = segment_using_column_scan(preprocessed_image, captcha_text, print_msg=print_img)\n",
    "    characters = None\n",
    "    \n",
    "    return characters, preprocessed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.2) YoloBoundingCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLO  2024-12-1 Python-3.10.15 torch-2.5.1+cpu CPU\n",
      "\n",
      "d:\\ML Final\\114-ML-Group37\\yolov9_2\\models\\experimental.py:243: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
      "Fusing layers... \n",
      "yolov9-c summary: 604 layers, 50698278 parameters, 0 gradients, 236.6 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[          8          27          39          76]\n",
      " [         66          22          92          75]\n",
      " [        144          21         170          76]\n",
      " [        172          23         200          71]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class YoloBoundingCreator:\n",
    "    def __init__(self):\n",
    "        self._weights = \"yolov9_2\\\\runs\\\\train\\\\exp\\\\weights\\\\best.pt\"  # 模型權重文件\n",
    "        self._imgsz = (640, 640)\n",
    "        self._device = select_device(device='cpu')\n",
    "        self._dnn = False\n",
    "        self._data = 'data\\datasets.yaml'\n",
    "        self._half = False\n",
    "        self._model = DetectMultiBackend(self._weights, device=self._device,  dnn=self._dnn, data=self._data, fp16=self._half)\n",
    "        self._stride, self._names, self._pt = self._model.stride, self._model.names, self._model.pt\n",
    "        self._imgsz = check_img_size(self._imgsz, s=self._stride)\n",
    "\n",
    "    def bouding_generator(self, source_image):\n",
    "        im0 = source_image\n",
    "        im = letterbox(im0, self._imgsz, stride=self._stride, auto=self._pt)[0]  # padded resize\n",
    "        im = im.transpose((2, 0, 1))[::-1]\n",
    "        im = np.ascontiguousarray(im)\n",
    "\n",
    "        img = torch.from_numpy(im).to(self._device)\n",
    "        img = img.half() if self._model.fp16 else img.float()\n",
    "        img /= 255.0\n",
    "        if len(img.shape) == 3:\n",
    "            img = img[None]\n",
    "\n",
    "        pred = self._model(img)\n",
    "        pred = pred[0][1]\n",
    "        pred = non_max_suppression(pred, 0.1, 0.45, classes=None, agnostic=False)\n",
    "\n",
    "        s = ''\n",
    "        for i, det in enumerate(pred):  # per image\n",
    "            s += '%gx%g ' % img.shape[2:]\n",
    "            annotator = Annotator(im0, line_width=1, example=str(self._names))\n",
    "            if len(det):\n",
    "                det[:, :4] = scale_boxes(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "                for c in det[:, 5].unique():\n",
    "                    n = (det[:, 5] == c).sum()\n",
    "                    s += f\"{n} {self._names[int(c)]}{'s' * (n > 1)}, \"\n",
    "                hide_labels = True\n",
    "                hide_conf = True\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    c = int(cls)\n",
    "                    label = None if hide_labels else (self._names[c] if hide_conf else f'{self._names[c]} {conf:.2f}')\n",
    "                    annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "\n",
    "        im0 = annotator.result()\n",
    "        bouding_box = det[:, :4].cpu().numpy()\n",
    "\n",
    "        # sort the bounding box by x1\n",
    "        bouding_box = bouding_box[bouding_box[:, 0].argsort()]\n",
    "\n",
    "        return bouding_box\n",
    "\n",
    "def draw_bounding_box(image, bounding_box):\n",
    "    \n",
    "    for i in range(len(bounding_box)):\n",
    "        x1, y1, x2, y2 = bounding_box[i]\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 1) # (0, 255, 0) is the color of the bounding box, 2 is the thickness of the bounding box\n",
    "\n",
    "    return image\n",
    "\n",
    "yolo_bounding_creator = YoloBoundingCreator()\n",
    "                \n",
    "# Testing\n",
    "source_path = \"dataset\\\\overlap\\\\val\\\\preprocess\\\\0.png\"\n",
    "source_image = cv2.imread(source_path)\n",
    "bouding_box = yolo_bounding_creator.bouding_generator(source_image)\n",
    "print(bouding_box)\n",
    "image_with_bounding_box = draw_bounding_box(source_image, bouding_box)\n",
    "# Save the image with bounding box\n",
    "cv2.imwrite(\"image_with_bounding_box.jpg\", image_with_bounding_box)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.3) Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_fixed_stride(image, model, input_shape, window_width, stride, confidence_threshold, index_to_char, verbose=0):\n",
    "    \"\"\"\n",
    "    Predict characters using fixed stride and select top 4 predictions with highest confidence and no overlapping regions.\n",
    "    Args:\n",
    "        image: np.array, input image.\n",
    "        model: trained model for prediction.\n",
    "        input_shape: tuple, (height, width) required by the model.\n",
    "        window_width: int, width of the sliding window.\n",
    "        stride: int, step size for sliding window horizontally.\n",
    "        confidence_threshold: float, minimum confidence to accept prediction.\n",
    "        index_to_char: dict, mapping from class index to character.\n",
    "        verbose: int, 0 or 1 for verbose.\n",
    "\n",
    "    Returns:\n",
    "        str, predicted text for the image.\n",
    "    \"\"\"\n",
    "    image_height, image_width = image.shape\n",
    "    predictions = []\n",
    "\n",
    "    # Sliding window with fixed stride\n",
    "    x_start = 0\n",
    "    while x_start + window_width <= image_width:\n",
    "        # Crop region\n",
    "        cropped_region = image[0:image_height, x_start:x_start + window_width]\n",
    "        region = preprocess_image(cropped_region, input_shape)\n",
    "\n",
    "        # Predict with model\n",
    "        probs = model.predict(region, verbose=verbose)\n",
    "        max_prob = np.max(probs)\n",
    "        if max_prob >= confidence_threshold:\n",
    "            predicted_class = np.argmax(probs)\n",
    "            predicted_char = index_to_char.get(predicted_class, '?')\n",
    "            predictions.append((predicted_char, max_prob, x_start, x_start + window_width))  # Save char, prob, start, end\n",
    "\n",
    "        x_start += stride  # Fixed stride\n",
    "\n",
    "    # Sort predictions by confidence (descending)\n",
    "    predictions = sorted(predictions, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Select top 4 predictions with no overlapping regions\n",
    "    selected_predictions = []\n",
    "    for char, prob, start, end in predictions:\n",
    "        # Check for overlap with already selected regions\n",
    "        if all(end <= sel_start or start >= sel_end for _, _, sel_start, sel_end in selected_predictions):\n",
    "            selected_predictions.append((char, prob, start, end))\n",
    "            if len(selected_predictions) == 4:  # Stop after selecting 4 predictions\n",
    "                break\n",
    "\n",
    "    # Sort selected predictions by position (start)\n",
    "    selected_predictions = sorted(selected_predictions, key=lambda x: x[2])\n",
    "\n",
    "    # Combine selected characters into a string\n",
    "    predicted_text = ''.join([char for char, _, _, _ in selected_predictions])\n",
    "\n",
    "    return predicted_text\n",
    "\n",
    "def get_char_region(image, bounding_box):\n",
    "    \"\"\"\n",
    "    Get the region of the character from the bounding box.\n",
    "\n",
    "    Args:\n",
    "        image: np.array, input image.\n",
    "        bounding_box: list, bounding box coordinates.\n",
    "\n",
    "    Returns:\n",
    "        np.array, region of the character.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bounding_box\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    # char_region = image[y1:y2, x1:x2] \n",
    "    # Keep the original height of the character\n",
    "    char_region = image[:, x1:x2]\n",
    "    return char_region\n",
    "\n",
    "def sliding_predict_and_validate(image_dir, labels_csv, model_path, input_shape, yolo_model ,confidence_threshold, index_to_char, print_img=False, predict_size='None'):\n",
    "    \"\"\"\n",
    "    Process multiple images, predict characters, and validate against ground truth.\n",
    "    We have two methods to predict characters:\n",
    "    1. Use the preprocess_captcha_image function to segment characters, and then predict each character.\n",
    "    2. Use the predict_with_multiple_windows or predict_with_fixed_stride function to predict characters directly.\n",
    "\n",
    "    We Use 2. if the characters are not segmented properly.\n",
    "\n",
    "    Args:\n",
    "        image_dir: str, directory containing image files.\n",
    "        labels_csv: str, path to the CSV file containing labels for each character.\n",
    "        model: trained model for prediction.\n",
    "        input_shape: tuple, (height, width) required by the model.\n",
    "        yolo_model: YoloBoundingCreator, the YOLO model for bounding box prediction.\n",
    "        confidence_threshold: float, threshold to accept predictions.\n",
    "        index_to_char: dict, mapping from class index to character.\n",
    "\n",
    "    Returns:\n",
    "        None (Displays validation results).\n",
    "    \"\"\"\n",
    "    # Predicition labels list\n",
    "    prediction_labels = []\n",
    "    # Ground truth labels list\n",
    "    ground_truth_labels = []\n",
    "\n",
    "    # Output path\n",
    "    output_path = \"prediction.txt\"\n",
    "\n",
    "    # Load labels\n",
    "    labels_df = pd.read_csv(labels_csv)\n",
    "    if not {'image', 'label'}.issubset(labels_df.columns):\n",
    "        raise ValueError(\"CSV file must contain 'image' and 'label' columns.\")\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    correct = 0\n",
    "    correct_combined = 0\n",
    "    total = 0\n",
    "    count_images = 0\n",
    "    total_images = predict_size\n",
    "    image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    # Progress bar start\n",
    "    progress_bar = tqdm.tqdm(total=total_images, position=0, leave=True)\n",
    "\n",
    "    # Write predictions to a file\n",
    "    f = open(output_path, 'w')\n",
    "    for image_path in image_paths:\n",
    "        image_id = os.path.basename(image_path)\n",
    "        if image_id not in labels_df['image'].values:\n",
    "            print(f\"Warning: No label found for image ID {image_id}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        ground_truth = labels_df.loc[labels_df['image'] == image_id, 'label'].values[0]\n",
    "        \n",
    "        # # Load the image\n",
    "        # image = cv2.imread(image_path) # Load image in BGR format\n",
    "        # # To black and white\n",
    "        # image = to_black_and_white(image, threshold=200, print_img=print_img)\n",
    "\n",
    "        # if image is None:\n",
    "        #     print(f\"Warning: Failed to load image {image_path}. Skipping.\")\n",
    "        #     continue\n",
    "\n",
    "        # <Method 1> Preprocess image and segment characters\n",
    "        characters, preprocessed_image = preprocess_captcha_image(image_path, ground_truth, print_img=print_img)\n",
    "\n",
    "        if print_img:\n",
    "            print(f\"Number of segmented characters: {len(characters)}\")\n",
    "\n",
    "        # if len(characters) == 4: # The characters are segmented properly\n",
    "        #     predicted_text = \"\"\n",
    "        #     for char_image in characters:\n",
    "        #         if print_img:\n",
    "        #             plt.imshow(char_image, cmap='gray')\n",
    "        #             plt.axis('off')\n",
    "        #             plt.title('Segmented Character')\n",
    "        #             plt.show()\n",
    "        #         region = preprocess_image(char_image, input_shape)\n",
    "        #         probs = model.predict(region, verbose=0)\n",
    "        #         predicted_class = np.argmax(probs)\n",
    "        #         predicted_char = index_to_char.get(predicted_class, '?')\n",
    "        #         predicted_text += predicted_char\n",
    "                \n",
    "        # else:\n",
    "        #     # <Method 2> Predict with fixed stride\n",
    "        #     predicted_text = predict_with_fixed_stride(\n",
    "        #         preprocessed_image, model, input_shape, window_width=20, stride=1, confidence_threshold=confidence_threshold, index_to_char=index_to_char\n",
    "        #     )\n",
    "\n",
    "        # <Method 3> Use the yolov9 model to get the bounding box of the characters, and then predict each character\n",
    "        # Remember to convert the image to BGR format\n",
    "        preprocessed_image = cv2.cvtColor(preprocessed_image, cv2.COLOR_GRAY2BGR)\n",
    "        bounding_boxes = yolo_model.bouding_generator(preprocessed_image)\n",
    "\n",
    "        predicted_text = \"\"\n",
    "        if len(bounding_boxes) > 0:\n",
    "            # Keep only the 4 bounding boxes with the highest confidence\n",
    "            if len(bounding_boxes) > 4:\n",
    "                bounding_boxes = bounding_boxes[:4]\n",
    "\n",
    "            for i, bounding_box in enumerate(bounding_boxes):\n",
    "                char_region = get_char_region(preprocessed_image, bounding_box)\n",
    "                char_region = cv2.cvtColor(char_region, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # if print_img:\n",
    "                #     # Save the character region\n",
    "                #     cv2.imwrite(f\"char_region_{i}.jpg\", char_region)\n",
    "\n",
    "                region = preprocess_image(char_region, input_shape)\n",
    "\n",
    "                # if print_img:\n",
    "                #     # Save the character region\n",
    "                #     cv2.imwrite(f\"char_region_p_{i}.jpg\", region)\n",
    "\n",
    "                probs = model.predict(region, verbose=0)\n",
    "                predicted_class = np.argmax(probs) # Get the class with the highest probability\n",
    "                predicted_char = index_to_char.get(predicted_class, '?')\n",
    "                predicted_text += predicted_char\n",
    "\n",
    "        # If the predicted text is 0 characters, that means we over process the image and the preprocessed image maybe all white\n",
    "        # We should use the original image to predict\n",
    "        else:\n",
    "            print(\"Warning: No bounding box found. Using the original image to predict.\")\n",
    "            image = cv2.imread(image_path)\n",
    "            image = to_black_and_white(image, threshold=200, print_img=print_img)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "            bounding_boxes = yolo_model.bouding_generator(image)\n",
    "\n",
    "            predicted_text = \"\"\n",
    "            # Keep only the 4 bounding boxes with the highest confidence\n",
    "            if len(bounding_boxes) > 4:\n",
    "                bounding_boxes = bounding_boxes[:4]\n",
    "\n",
    "            for i, bounding_box in enumerate(bounding_boxes):\n",
    "                char_region = get_char_region(image, bounding_box)\n",
    "                char_region = cv2.cvtColor(char_region, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # if print_img:\n",
    "                #     # Save the character region\n",
    "                #     cv2.imwrite(f\"char_region_{i}.jpg\", char_region)\n",
    "\n",
    "                region = preprocess_image(char_region, input_shape)\n",
    "\n",
    "                # if print_img:\n",
    "                #     # Save the character region\n",
    "                #     cv2.imwrite(f\"char_region_p_{i}.jpg\", region)\n",
    "\n",
    "                probs = model.predict(region, verbose=0)\n",
    "                predicted_class = np.argmax(probs)\n",
    "                predicted_char = index_to_char.get(predicted_class, '?')\n",
    "                predicted_text += predicted_char\n",
    "\n",
    "        # Compare predictions with the ground truth\n",
    "        for pred_char, gt_char in zip_longest(predicted_text, ground_truth, fillvalue=' '):\n",
    "            prediction_labels.append(pred_char) # if the prediction is empty, we add a space\n",
    "            ground_truth_labels.append(gt_char)\n",
    "            if pred_char == gt_char:\n",
    "                correct += 1\n",
    "\n",
    "        total += len(ground_truth)\n",
    "        count_images += 1\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        # print(f\"Image: {image_id}, Ground Truth: '{ground_truth}', Prediction: '{predicted_text}'\")\n",
    "        # Write the prediction to a file\n",
    "        f.write(f\"{image_id}, Ground Truth: '{ground_truth}', Prediction: '{predicted_text}'\\n\")\n",
    "        if predicted_text == ground_truth:\n",
    "            correct_combined += 1\n",
    "\n",
    "        if type(predict_size) == int and count_images >= predict_size:\n",
    "            break\n",
    "        \n",
    "        if count_images % 100 == 0:\n",
    "            # Accuracy for every 100 images\n",
    "            accuracy = (correct / total) * 100\n",
    "            print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "            accuracy_combined = (correct_combined / count_images) * 100\n",
    "            print(f\"Combined Accuracy: {accuracy_combined:.2f}%\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    f.close()\n",
    "    # Display overall accuracy\n",
    "    if total > 0:\n",
    "        accuracy = (correct / total) * 100\n",
    "        print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(\"No valid images processed.\")\n",
    "\n",
    "    # Display overall accuracy\n",
    "    if count_images > 0:\n",
    "        combined_accuracy = (correct_combined / count_images) * 100\n",
    "        print(f\"Combined Accuracy: {combined_accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(\"No valid images processed.\")\n",
    "\n",
    "    return prediction_labels, ground_truth_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.3) Parallel Prediction (DO NOT USE THIS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_predict_and_validate_parallel(image_dir, labels_csv, model_path, input_shape, windows, confidence_threshold, index_to_char, print_img=False, predict_size='None'):\n",
    "    \"\"\"\n",
    "    Parallel processing of multiple images for prediction and validation.\n",
    "    Args:\n",
    "        image_dir: str, directory containing image files.\n",
    "        labels_csv: str, path to the CSV file containing labels for each character.\n",
    "        model_path: str, path to the trained model file.\n",
    "        input_shape: tuple, (height, width) required by the model.\n",
    "        windows: list of tuples, each containing (window_width, stride).\n",
    "        confidence_threshold: float, threshold to accept predictions.\n",
    "        index_to_char: dict, mapping from class index to character.\n",
    "        print_img: bool, whether to display images during processing.\n",
    "\n",
    "    Returns:\n",
    "        None (Displays validation results).\n",
    "    \"\"\"\n",
    "    # Load labels\n",
    "    labels_df = pd.read_csv(labels_csv)\n",
    "    if not {'image', 'label'}.issubset(labels_df.columns):\n",
    "        raise ValueError(\"CSV file must contain 'image' and 'label' columns.\")\n",
    "\n",
    "    image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    # Prepare arguments for each image\n",
    "    args_list = []\n",
    "    count_images = 0\n",
    "    for image_path in image_paths:\n",
    "        args = (image_path, labels_df, model_path, input_shape, windows, confidence_threshold, index_to_char, print_img)\n",
    "        args_list.append(args)\n",
    "        count_images += 1\n",
    "        if type(predict_size) == int and count_images >= predict_size:\n",
    "            break\n",
    "\n",
    "    # 使用多进程池\n",
    "    num_processes = 2\n",
    "    results = []\n",
    "    with multiprocessing.Pool(processes=num_processes, initializer=init_process, initargs=(model_path,)) as pool:\n",
    "        results = pool.map(process_single_image, args_list)\n",
    "\n",
    "    # 汇总结果\n",
    "    total_correct = 0\n",
    "    total_characters = 0\n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            _, _, _, correct, total = result\n",
    "            total_correct += correct\n",
    "            total_characters += total\n",
    "\n",
    "    # Display the results\n",
    "    \n",
    "\n",
    "    # Display overall accuracy\n",
    "    if total_characters > 0:\n",
    "        accuracy = (total_correct / total_characters) * 100\n",
    "        print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(\"No valid images processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.4) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/5000 [01:37<1:13:36,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 93.00%\n",
      "Combined Accuracy: 76.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 200/5000 [03:05<1:10:31,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 91.25%\n",
      "Combined Accuracy: 75.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 300/5000 [04:30<1:03:26,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 91.50%\n",
      "Combined Accuracy: 75.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 400/5000 [05:50<1:01:09,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.25%\n",
      "Combined Accuracy: 77.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 500/5000 [07:11<59:12,  1.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.50%\n",
      "Combined Accuracy: 78.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 600/5000 [08:32<57:47,  1.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.50%\n",
      "Combined Accuracy: 78.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 700/5000 [09:51<57:23,  1.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.25%\n",
      "Combined Accuracy: 78.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 800/5000 [11:12<54:57,  1.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.12%\n",
      "Combined Accuracy: 77.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 900/5000 [12:31<53:26,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.36%\n",
      "Combined Accuracy: 77.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1000/5000 [13:50<52:59,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.30%\n",
      "Combined Accuracy: 77.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1100/5000 [15:09<51:08,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.36%\n",
      "Combined Accuracy: 77.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1200/5000 [16:28<50:17,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.42%\n",
      "Combined Accuracy: 77.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1300/5000 [17:46<48:02,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.40%\n",
      "Combined Accuracy: 77.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1400/5000 [19:06<50:11,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.39%\n",
      "Combined Accuracy: 77.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1500/5000 [20:26<46:13,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.42%\n",
      "Combined Accuracy: 77.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 1600/5000 [21:46<44:33,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.48%\n",
      "Combined Accuracy: 77.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1700/5000 [23:10<43:43,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.62%\n",
      "Combined Accuracy: 78.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 1800/5000 [24:29<41:59,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.79%\n",
      "Combined Accuracy: 78.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1900/5000 [25:49<40:51,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.79%\n",
      "Combined Accuracy: 78.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2000/5000 [27:08<39:36,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.73%\n",
      "Combined Accuracy: 78.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 2100/5000 [28:28<38:21,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.67%\n",
      "Combined Accuracy: 78.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 2200/5000 [29:48<36:34,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.60%\n",
      "Combined Accuracy: 78.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 2300/5000 [31:07<35:52,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.70%\n",
      "Combined Accuracy: 78.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 2400/5000 [32:27<34:06,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.70%\n",
      "Combined Accuracy: 78.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2500/5000 [33:47<32:58,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.75%\n",
      "Combined Accuracy: 78.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2600/5000 [35:06<32:36,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.72%\n",
      "Combined Accuracy: 78.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 2700/5000 [36:25<30:28,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.61%\n",
      "Combined Accuracy: 78.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 2800/5000 [37:46<29:04,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.57%\n",
      "Combined Accuracy: 78.43%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 2900/5000 [39:05<27:47,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.47%\n",
      "Combined Accuracy: 78.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3000/5000 [40:20<22:03,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.42%\n",
      "Combined Accuracy: 77.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 3100/5000 [41:29<23:01,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.41%\n",
      "Combined Accuracy: 78.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 3200/5000 [42:31<19:24,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.45%\n",
      "Combined Accuracy: 78.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 3300/5000 [43:37<19:50,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.50%\n",
      "Combined Accuracy: 78.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3400/5000 [44:43<16:56,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.50%\n",
      "Combined Accuracy: 78.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3500/5000 [45:42<14:51,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.48%\n",
      "Combined Accuracy: 78.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 3600/5000 [46:42<13:55,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.48%\n",
      "Combined Accuracy: 78.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 3700/5000 [47:41<12:35,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.50%\n",
      "Combined Accuracy: 78.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 3800/5000 [48:40<11:43,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.51%\n",
      "Combined Accuracy: 78.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 3900/5000 [49:39<10:35,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.45%\n",
      "Combined Accuracy: 78.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4000/5000 [50:42<11:50,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.42%\n",
      "Combined Accuracy: 78.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 4100/5000 [51:33<07:39,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.46%\n",
      "Combined Accuracy: 78.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 4200/5000 [52:24<06:44,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.52%\n",
      "Combined Accuracy: 78.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 4300/5000 [53:14<05:45,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.56%\n",
      "Combined Accuracy: 78.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 4400/5000 [54:04<04:55,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.57%\n",
      "Combined Accuracy: 78.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4500/5000 [54:56<04:58,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.56%\n",
      "Combined Accuracy: 78.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 4600/5000 [55:46<03:20,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.56%\n",
      "Combined Accuracy: 78.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 4700/5000 [56:36<02:27,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.59%\n",
      "Combined Accuracy: 78.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 4800/5000 [57:26<01:54,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.55%\n",
      "Combined Accuracy: 78.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 4900/5000 [58:16<00:50,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.57%\n",
      "Combined Accuracy: 78.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [59:05<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 92.53%\n",
      "Combined Accuracy: 78.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "test_image_paths = \"dataset\\\\overlap\\\\test\\\\original\"  # Directory containing images\n",
    "test_labels_csv = \"dataset\\\\overlap\\\\test\\\\labels\\\\labels.csv\"  # Path to the CSV file with labels\n",
    "\n",
    "# Parameters for sliding window\n",
    "confidence_threshold = 0.8 # Minimum confidence required to accept a prediction\n",
    "input_shape = (90, 30)      # Model's expected input shape\n",
    "\n",
    "# Define your index-to-character mapping\n",
    "index_to_char = {i: chr(i + ord('0')) for i in range(10)}  # 0-9\n",
    "index_to_char.update({i + 10: chr(i + ord('a')) for i in range(26)})  # 10-35\n",
    "index_to_char.update({i + 36: chr(i + ord('A')) for i in range(26)})  # 36-61\n",
    "\n",
    "prediction_labels, ground_truth_labels = sliding_predict_and_validate(test_image_paths, test_labels_csv, \"captcha_model.h5\", input_shape, yolo_bounding_creator, confidence_threshold, index_to_char, print_img=False, predict_size=5000)\n",
    "# sliding_predict_and_validate_parallel(image_paths, labels_csv, \"captcha_model.h5\", input_shape, windows, confidence_threshold, index_to_char, print_img=False, predict_size=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2.2.5) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Micro: 0.9253\n",
      "F1 Macro: 0.9251\n",
      "F1 Weighted: 0.9256\n"
     ]
    }
   ],
   "source": [
    "def F1_score(records):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score for a prediction compared to ground truth.\n",
    "    Args:\n",
    "        records: A txt file containing the prediction and ground truth for each image.\n",
    "                 Each line: \"image_id, Ground Truth: 'XXXX', Prediction: 'XXXX'\"\n",
    "    Returns:    \n",
    "        F1 score.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "\n",
    "    with open(records, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                fields = line.split(',')\n",
    "                ground_truth = fields[1].split('\\'')[1]\n",
    "                prediction = fields[2].split('\\'')[1]\n",
    "                \n",
    "                if len(prediction) == 4:\n",
    "                    for i in range(4):\n",
    "                        predictions.append(prediction[i])\n",
    "                        ground_truths.append(ground_truth[i])\n",
    "                else:\n",
    "                    for i in range(4):\n",
    "                        predictions.append(' ') # Add a space for missing characters\n",
    "                        ground_truths.append(ground_truth[i])\n",
    "\n",
    "    # Calculate F1 score\n",
    "    classes = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 ')\n",
    "    le = LabelEncoder()\n",
    "    le.fit(classes)\n",
    "\n",
    "    y_true = le.transform(ground_truths)\n",
    "    y_pred = le.transform(predictions)\n",
    "\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    return f1_micro, f1_macro, f1_weighted\n",
    "\n",
    "def F1_score_with_labels(predictions, ground_truths):\n",
    "    \"\"\"\n",
    "    Calculate the F1 score for a prediction compared to ground truth.\n",
    "    Args:\n",
    "        records: A txt file containing the prediction and ground truth for each image.\n",
    "                 Each line: \"image_id, Ground Truth: 'XXXX', Prediction: 'XXXX'\"\n",
    "    Returns:    \n",
    "        F1 score.\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(predictions) == len(ground_truths), \"Number of predictions and ground truths must be the same.\"\n",
    "    # Calculate F1 score\n",
    "    classes = list('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 ')\n",
    "    le = LabelEncoder()\n",
    "    le.fit(classes)\n",
    "\n",
    "    y_true = le.transform(ground_truths)\n",
    "    y_pred = le.transform(predictions)\n",
    "\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "    return f1_micro, f1_macro, f1_weighted\n",
    "\n",
    "# Testing\n",
    "# records = \"prediction.txt\"\n",
    "# f1_micro, f1_macro, f1_weighted = F1_score(records)\n",
    "f1_micro, f1_macro, f1_weighted = F1_score_with_labels(prediction_labels, ground_truth_labels)\n",
    "print(f\"F1 Micro: {f1_micro:.4f}\")\n",
    "print(f\"F1 Macro: {f1_macro:.4f}\")\n",
    "print(f\"F1 Weighted: {f1_weighted:.4f}\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "image_paths = \"dataset\\\\overlap\\\\test\\\\original\"\n",
    "labels_csv = \"dataset\\\\overlap\\\\test\\\\labels\\\\labels.csv\"\n",
    "\n",
    "# Parameters for sliding window\n",
    "confidence_threshold = 0.8 # Minimum confidence required to accept a prediction\n",
    "input_shape = (90, 30)      # Model's expected input shape\n",
    "\n",
    "# Define your index-to-character mapping\n",
    "index_to_char = {i: chr(i + ord('0')) for i in range(10)}  # 0-9\n",
    "index_to_char.update({i + 10: chr(i + ord('a')) for i in range(26)})  # 10-35\n",
    "index_to_char.update({i + 36: chr(i + ord('A')) for i in range(26)})  # 36-61\n",
    "\n",
    "sliding_predict_and_validate(image_paths, labels_csv, \"captcha_model.h5\", input_shape, yolo_bounding_creator, confidence_threshold, index_to_char, print_img=False, predict_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
